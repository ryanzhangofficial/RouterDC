{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0ae3937852d7d4b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Handling GSM8K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "849c65d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"./datasets/split2_model7\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102579b7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e574a4d570f91c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T13:50:26.765018Z",
     "start_time": "2024-04-17T13:50:23.629507Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_list = [  \n",
    "                ['mistralai','Mistral-7B-v0.1'],\n",
    "                ['meta-math','MetaMath-Mistral-7B'],\n",
    "                ['itpossible','Chinese-Mistral-7B-v0.1'],\n",
    "                [\"HuggingFaceH4\",\"zephyr-7b-beta\"],\n",
    "                [\"cognitivecomputations\",\"dolphin-2.6-mistral-7b\"],\n",
    "                [\"meta-llama\",\"Meta-Llama-3-8B\"],\n",
    "                [\"cognitivecomputations\",\"dolphin-2.9-llama3-8b\"],\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4891d237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ./output/humaneval/Arithmo2-Mistral-7B_humaneval.json\n",
    "\n",
    "# for mbpp\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "task = \"mbpp\"\n",
    "\n",
    "mbpp_data = pd.read_parquet(\"./LLM_datasets/mbpp/full/test-00000-of-00001.parquet\",engine='pyarrow')\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    model_pre = model_info[0]\n",
    "    model = model_info[1]\n",
    "    with open(f\"./output/mbpp/{model}/log_samples.json\",'r') as f:\n",
    "        json_file = json.load(f)\n",
    "    json_file = json_file['mbpp']\n",
    "    for index, instance in json_file.items():\n",
    "        correct = 0\n",
    "        for each_resps in instance:\n",
    "            correct += 1 if each_resps[1]['passed'] == True else 0\n",
    "        acc = correct / len(instance)\n",
    "        description = mbpp_data.iloc[int(index)][\"text\"]\n",
    "        test_example = mbpp_data.iloc[int(index)][\"test_list\"][0]\n",
    "        question = f'\"\"\"\\n{description}\\n{test_example}\\n\"\"\"\\n'\n",
    "        if model_index == 0:\n",
    "            output_data.append({\"question\": question, \"scores\": {model_pre + \"/\" + model: acc}})\n",
    "        else:\n",
    "            output_data[int(index)][\"scores\"][model_pre+\"/\" + model] = acc\n",
    "\n",
    "print(len(output_data))\n",
    "\n",
    "with open(os.path.join(output_file_path, 'mbpp.json'), 'w') as f:\n",
    "    json.dump(output_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "416871ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/* Check if in given list of numbers, are any two numbers closer to each other than\n",
      "  given threshold.\n",
      "  >>> hasCloseElements([1.0, 2.0, 3.0], 0.5)\n",
      "  false\n",
      "  >>> hasCloseElements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
      "  true\n",
      "  */\n",
      "const hasCloseElements = (numbers, threshold) => {\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ./output/humaneval/Arithmo2-Mistral-7B_humaneval.json\n",
    "\n",
    "# for javascript\n",
    "model_list = [  \n",
    "                ['mistralai','Mistral-7B-v0.1'],\n",
    "                ['meta-math','MetaMath-Mistral-7B'],\n",
    "                ['itpossible','Chinese-Mistral-7B-v0.1'],\n",
    "                [\"HuggingFaceH4\",\"zephyr-7b-beta\"],\n",
    "                [\"cognitivecomputations\",\"dolphin-2.6-mistral-7b\"],\n",
    "                [\"meta-llama\",\"Meta-Llama-3-8B\"],\n",
    "                [\"cognitivecomputations\",\"dolphin-2.9-llama3-8b\"],\n",
    "                ]\n",
    "\n",
    "output_file_path = \"./datasets/split2_model7\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "task = \"javascript\"\n",
    "\n",
    "javascript_data = pd.read_json(\"./LLM_datasets/bigcode/humanevalpack/data/js/data/humanevalpack.jsonl\", lines=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3e2b791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    model_pre = model_info[0]\n",
    "    model = model_info[1]\n",
    "    with open(f\"./output/humanevalpack_js/{model}/log_samples.json\",'r') as f:\n",
    "        json_file = json.load(f)\n",
    "    json_file = json_file['humanevalsynthesize-js']\n",
    "    for index, instance in json_file.items():\n",
    "        correct = 0\n",
    "        for each_resps in instance:\n",
    "            correct += 1 if each_resps[1]['passed'] == True else 0\n",
    "        acc = correct / len(instance)\n",
    "        question = javascript_data.iloc[int(index)][\"prompt\"][0]\n",
    "        if model_index == 0:\n",
    "            output_data.append({\"question\": question, \"scores\": {model_pre + \"/\" + model: acc}})\n",
    "        else:\n",
    "            output_data[int(index)][\"scores\"][model_pre + \"/\" + model] = acc\n",
    "\n",
    "print(len(output_data))\n",
    "\n",
    "with open(os.path.join(output_file_path, 'javascript.json'), 'w') as f:\n",
    "    json.dump(output_data, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af05a285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# for gsm8k\n",
    "\n",
    "task = \"gsm8k_train\"\n",
    "# task = \"gsm8k-repeat10\"\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    model_pre = model_info[0]\n",
    "    model = model_info[1]\n",
    "    with open(f\"./output/gsm8k_train-t0.2/{model}/pretrained____data__home__chensh__data__huggingface_model__{model_pre}__{model}_{task}.jsonl\", 'r') as f:\n",
    "        json_file = json.load(f)\n",
    "    for i in range(len(json_file)):\n",
    "        # extract answer_list & answer\n",
    "        resps = json_file[i]['resps'][0]\n",
    "        pattern = \"#### (\\\\-?[0-9\\\\.\\\\,]+)\"\n",
    "        ans_list = [re.search(pattern, resp).group(1) if re.search(pattern, resp) else None for resp in resps]\n",
    "        target = re.search(pattern, json_file[i]['target']).group(1)\n",
    "         # compare with origin answer & get score\n",
    "        correct = 0\n",
    "        for ans in ans_list:\n",
    "            correct += 1 if ans == target else 0 \n",
    "        acc = correct / len(ans_list)\n",
    "        if model_index == 0:\n",
    "            output_data.append({\"question\":json_file[i]['doc'][\"question\"], \"scores\": {model_pre+\"/\" + model: acc}})\n",
    "        else:\n",
    "            output_data[i][\"scores\"][model_pre+\"/\" + model] = acc\n",
    "            \n",
    "output_data = output_data[:int(len(output_data)/2)]\n",
    "with open(os.path.join(output_file_path, 'gsm8k-train.json'), 'w') as f:\n",
    "    json.dump(output_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23efe811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b6469276e69aaf6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3fcaecf816259d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T02:04:17.747956Z",
     "start_time": "2024-04-18T02:03:59.839784Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# for mmlu (multiple choice problem)\n",
    "\n",
    "task_list = ['abstract_algebra', 'anatomy', 'astronomy', 'business_ethics', 'clinical_knowledge', 'college_biology', 'college_chemistry', 'college_computer_science', 'college_mathematics', 'college_medicine', 'college_physics', 'computer_security', 'conceptual_physics', 'econometrics', 'electrical_engineering', 'elementary_mathematics', 'formal_logic', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_computer_science', 'high_school_european_history', 'high_school_geography', 'high_school_government_and_politics', 'high_school_macroeconomics', 'high_school_mathematics', 'high_school_microeconomics', 'high_school_physics', 'high_school_psychology', 'high_school_statistics', 'high_school_us_history', 'high_school_world_history', 'human_aging', 'human_sexuality', 'international_law', 'jurisprudence', 'logical_fallacies', 'machine_learning', 'management', 'marketing', 'medical_genetics', 'miscellaneous', 'moral_disputes', 'moral_scenarios', 'nutrition', 'philosophy', 'prehistory', 'professional_accounting', 'professional_law', 'professional_medicine', 'professional_psychology', 'public_relations', 'security_studies', 'sociology', 'us_foreign_policy', 'virology', 'world_religions']\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    count = 0\n",
    "    choices_head = ['A. ', 'B. ', 'C. ', 'D. ']\n",
    "    for task in task_list:\n",
    "        model_pre = model_info[0]\n",
    "        model = model_info[1]\n",
    "        with open(f\"./output/mmlu_5shot/{model}/pretrained____data__home__chensh__data__huggingface_model__{model_pre}__{model}_mmlu_{task}.jsonl\", 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        for i in range(len(json_file)):\n",
    "            resps = np.array(json_file[i]['resps'])\n",
    "            log_probability = resps[:,0,0]\n",
    "            probability = np.exp(log_probability)\n",
    "            sum_probability = np.sum(probability)\n",
    "            probability = probability / sum_probability\n",
    "            score = probability[json_file[i]['target']] if json_file[i]['acc'] == 1 else 0\n",
    "\n",
    "            if model_index == 0:\n",
    "                output_data.append(\n",
    "                    {\"question\":json_file[i]['doc'][\"question\"] + \n",
    "                                \"\".join([\"\\n\" + choices_head[j] + choice for j, choice in enumerate(json_file[i]['doc']['choices'])]), \"scores\": {model_pre+\"/\" + model: score}}\n",
    "                )\n",
    "            else:\n",
    "                output_data[count][\"scores\"][model_pre+\"/\" + model] = score\n",
    "                count += 1\n",
    "\n",
    "train_split_index = random.sample(range(len(output_data)), len(output_data))\n",
    "output_data = [output_data[index] for index in train_split_index]\n",
    "train_split = output_data[:int(0.7* len(output_data))]\n",
    "test_split = output_data[int(0.7* len(output_data)):]\n",
    "\n",
    "\n",
    "# output training data as a json file\n",
    "with open(os.path.join(output_file_path, f'mmlu_train.json'), 'w') as f:\n",
    "    json.dump(train_split, f)\n",
    "\n",
    "with open(os.path.join(output_file_path, f'mmlu_test.json'), 'w') as f:\n",
    "    json.dump(test_split, f) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90364170b5ba9a5e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Humaneval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c1583723bf47146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T15:18:06.488617Z",
     "start_time": "2024-04-17T15:18:06.362495Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ./output/humaneval/Arithmo2-Mistral-7B_humaneval.json\n",
    "\n",
    "# for humaneval\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "task = \"humaneval\"\n",
    "\n",
    "humaneval_data = pd.read_parquet(\"./LLM_datasets/openai_humaneval/openai_humaneval/test-00000-of-00001.parquet\",engine='pyarrow')\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    model_pre = model_info[0]\n",
    "    model = model_info[1]\n",
    "    with open(f\"./output/humaneval/{model}/log_samples.json\",'r') as f:\n",
    "        json_file = json.load(f)\n",
    "    json_file = json_file['humaneval']\n",
    "    for index, instance in json_file.items():\n",
    "        correct = 0\n",
    "        for each_resps in instance:\n",
    "            correct += 1 if each_resps[1]['passed'] == True else 0\n",
    "        acc = correct / len(instance)\n",
    "        question = humaneval_data.iloc[int(index)]['prompt']\n",
    "        if model_index == 0:\n",
    "            output_data.append({\"question\": question, \"scores\": {model_pre + \"/\" + model: acc}})\n",
    "        else:\n",
    "            output_data[int(index)][\"scores\"][model_pre+\"/\" + model] = acc\n",
    "\n",
    "train_split_index = random.sample(range(len(output_data)), len(output_data))\n",
    "output_data = [output_data[index] for index in train_split_index]\n",
    "train_split = output_data[:115]\n",
    "test_split = output_data[115:]\n",
    "\n",
    "with open(os.path.join(output_file_path, 'humaneval_train.json'), 'w') as f:\n",
    "    json.dump(train_split, f)\n",
    "\n",
    "with open(os.path.join(output_file_path, 'humaneval_test.json'), 'w') as f:\n",
    "    json.dump(test_split, f) \n",
    "\n",
    "print(len(train_split))\n",
    "print(len(test_split))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110794afd5d57f46",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ARC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2b5f93481da8d7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T13:59:58.639353Z",
     "start_time": "2024-04-17T13:59:57.784770Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(100)\n",
    "\n",
    "# for arc (multiple choice problem)\n",
    "\n",
    "# task = \"arc_easy\"\n",
    "task = \"arc_challenge\"\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    count = 0\n",
    "    model_pre = model_info[0]\n",
    "    model = model_info[1]\n",
    "    with open(f\"./output/{task}/{model}/pretrained____data__home__chensh__data__huggingface_model__{model_pre}__{model}_{task}.jsonl\", 'r') as f:\n",
    "        json_file = json.load(f)\n",
    "    for i in range(len(json_file)):\n",
    "        resps = np.array(json_file[i]['resps'])\n",
    "        log_probability = resps[:,0,0]\n",
    "        probability = np.exp(log_probability)\n",
    "        sum_probability = np.sum(probability)\n",
    "        probability = probability / sum_probability\n",
    "        score = probability[json_file[i]['target']] if json_file[i]['acc'] == 1 else 0\n",
    "\n",
    "        if model_index == 0:\n",
    "            output_data.append(\n",
    "                {\"question\": \"Question:\" + json_file[i]['doc'][\"question\"] + \"\\nAnswer:\", \"scores\": {model_pre+\"/\" + model: score}}\n",
    "            )\n",
    "        else:\n",
    "            output_data[count][\"scores\"][model_pre + \"/\" + model] = score\n",
    "            count += 1\n",
    "\n",
    "train_split_index = random.sample(range(len(output_data)), len(output_data))\n",
    "output_data = [output_data[index] for index in train_split_index]\n",
    "train_split = output_data[:int(0.7* len(output_data))]\n",
    "test_split = output_data[int(0.7* len(output_data)):]\n",
    "\n",
    "with open(os.path.join(output_file_path, f'{task}_train.json'), 'w') as f:\n",
    "    json.dump(train_split, f)\n",
    "\n",
    "with open(os.path.join(output_file_path, f'{task}_test.json'), 'w') as f:\n",
    "    json.dump(test_split, f) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772eaa208a3fa8dd",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Hellaswag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "361e7844c515fbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T02:07:17.587540Z",
     "start_time": "2024-04-18T02:06:40.661765Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Removing ice from car: Then, the man writes over the snow covering the window of a car, and a woman wearing winter clothes smiles. Then',\n",
       " 'scores': {'mistralai/Mistral-7B-v0.1': 0.9999965516457299,\n",
       "  'meta-math/MetaMath-Mistral-7B': 0.9997326286037237,\n",
       "  'teknium/OpenHermes-2.5-Mistral-7B': 0.9999999632149246,\n",
       "  'WizardLM/WizardMath-7B-V1.1': 0.9999973056897147,\n",
       "  'itpossible/Chinese-Mistral-7B-v0.1': 0.9999980523735081,\n",
       "  'HuggingFaceH4/zephyr-7b-beta': 0.9999996688090383,\n",
       "  'cognitivecomputations/dolphin-2.6-mistral-7b': 0.9999998706397251,\n",
       "  'NousResearch/Hermes-2-Pro-Mistral-7B': 0.9999999561210721,\n",
       "  'cognitivecomputations/dolphin-2.2.1-mistral-7b': 0.9999994994896325,\n",
       "  'abhishekchohan/mistral-7B-forest-dpo': 0.9999999418205058}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# for hellaswag (multiple choice problem)\n",
    "\n",
    "task = \"hellaswag\"\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.strip()\n",
    "    # NOTE: Brackets are artifacts of the WikiHow dataset portion of HellaSwag.\n",
    "    text = text.replace(\" [title]\", \". \")\n",
    "    text = re.sub(\"\\\\[.*?\\\\]\", \"\", text)\n",
    "    text = text.replace(\"  \", \" \")\n",
    "    return text\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    count = 0\n",
    "    model_pre = model_info[0]\n",
    "    model = model_info[1]\n",
    "    with open(f\"./output/{task}_train/{model}/pretrained____data__home__chensh__data__huggingface_model__{model_pre}__{model}_{task}.jsonl\", 'r') as f:\n",
    "        json_file = json.load(f)\n",
    "    for i in range(len(json_file)):\n",
    "        resps = np.array(json_file[i]['resps'])\n",
    "        log_probability = resps[:,0,0]\n",
    "        probability = np.exp(log_probability)\n",
    "        sum_probability = np.sum(probability)\n",
    "        probability = probability / sum_probability\n",
    "        score = probability[json_file[i]['target']] if json_file[i]['acc_norm'] == 1 else 0\n",
    "        question = json_file[i]['doc'][\"ctx_a\"] + \" \" + json_file[i]['doc'][\"ctx_b\"].capitalize()\n",
    "        question = preprocess(json_file[i]['doc'][\"activity_label\"] + \": \" + question)\n",
    "\n",
    "        if model_index == 0:\n",
    "            output_data.append(\n",
    "                {\"question\": question, \"scores\": {model_pre+\"/\" + model: score}}\n",
    "            )\n",
    "        else:\n",
    "            output_data[count][\"scores\"][model_pre + \"/\" + model] = score\n",
    "            count += 1\n",
    "       \n",
    "# output training data as a json file\n",
    "# output_data = output_data[:1000]\n",
    "with open(os.path.join(output_file_path, f'hellaswag_validation.json'), 'w') as f:\n",
    "    json.dump(output_data, f)\n",
    "\n",
    "# len(output_data)\n",
    "\n",
    "output_data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeaaae8b70e936",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# C-Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd5f74e0465a8cf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T14:01:44.697641Z",
     "start_time": "2024-04-17T14:01:43.250187Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random  \n",
    "\n",
    "# for ceval (multiple choice problem)\n",
    "\n",
    "task_list = ['computer_network', 'operating_system', 'computer_architecture', 'college_programming', 'college_physics', 'college_chemistry', 'advanced_mathematics', 'probability_and_statistics', 'discrete_mathematics', 'electrical_engineer', 'metrology_engineer', 'high_school_mathematics', 'high_school_physics', 'high_school_chemistry', 'high_school_biology', 'middle_school_mathematics', 'middle_school_biology', 'middle_school_physics', 'middle_school_chemistry', 'veterinary_medicine', 'college_economics', 'business_administration', 'marxism', 'mao_zedong_thought', 'education_science', 'teacher_qualification', 'high_school_politics', 'high_school_geography', 'middle_school_politics', 'middle_school_geography', 'modern_chinese_history', 'ideological_and_moral_cultivation', 'logic', 'law', 'chinese_language_and_literature', 'art_studies', 'professional_tour_guide', 'legal_professional', 'high_school_chinese', 'high_school_history', 'middle_school_history', 'civil_servant', 'sports_science', 'plant_protection', 'basic_medicine', 'clinical_medicine', 'urban_and_rural_planner', 'accountant', 'fire_engineer', 'environmental_impact_assessment_engineer', 'tax_accountant', 'physician']\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    count = 0\n",
    "    choices_head = ['A', 'B', 'C', 'D']\n",
    "    for task in task_list:\n",
    "        model_pre = model_info[0]\n",
    "        model = model_info[1]\n",
    "        with open(f\"./output/ceval-validation/{model}/pretrained____data__home__chensh__data__huggingface_model__{model_pre}__{model}_ceval-valid_{task}.jsonl\", 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        for i in range(len(json_file)):\n",
    "            resps = np.array(json_file[i]['resps'])\n",
    "            log_probability = resps[:,0,0]\n",
    "            probability = np.exp(log_probability)\n",
    "            sum_probability = np.sum(probability)\n",
    "            probability = probability / sum_probability\n",
    "            score = probability[json_file[i]['target']] if json_file[i]['acc'] == 1 else 0\n",
    "\n",
    "            if model_index == 0:\n",
    "                output_data.append(\n",
    "                    {\"question\":json_file[i]['doc'][\"question\"] + \n",
    "                                \"\".join([\"\\n\" + choices_head[j] + \". \" + json_file[i]['doc'][choices_head[j]] for j in range(4)]) + \"答案：\", \"scores\": {model_pre+\"/\" + model: score}}\n",
    "                )\n",
    "            else:\n",
    "                output_data[count][\"scores\"][model_pre+\"/\" + model] = score\n",
    "                count += 1\n",
    "\n",
    "\n",
    "# output training data as a json file\n",
    "with open(os.path.join(output_file_path, 'ceval.json'), 'w') as f:\n",
    "    json.dump(output_data, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee1b9ba25abb7b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CMMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "852a348525a5d031",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T16:24:02.965837Z",
     "start_time": "2024-04-17T16:23:54.786990Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(52)\n",
    "\n",
    "task_list = ['agronomy', 'anatomy', 'ancient_chinese', 'arts', 'astronomy', 'business_ethics', 'chinese_civil_service_exam', 'chinese_driving_rule', 'chinese_food_culture', 'chinese_foreign_policy', 'chinese_history', 'chinese_literature', 'chinese_teacher_qualification', 'clinical_knowledge', 'college_actuarial_science', 'college_education', 'college_engineering_hydrology', 'college_law', 'college_mathematics', 'college_medical_statistics', 'college_medicine', 'computer_science', 'computer_security', 'conceptual_physics', 'construction_project_management', 'economics', 'education', 'electrical_engineering', 'elementary_chinese', 'elementary_commonsense', 'elementary_information_and_technology', 'elementary_mathematics', 'ethnology', 'food_science', 'genetics', 'global_facts', 'high_school_biology', 'high_school_chemistry', 'high_school_geography', 'high_school_mathematics', 'high_school_physics', 'high_school_politics', 'human_sexuality', 'international_law', 'journalism', 'jurisprudence', 'legal_and_moral_basis', 'logical', 'machine_learning', 'management', 'marketing', 'marxist_theory', 'modern_chinese', 'nutrition', 'philosophy', 'professional_accounting', 'professional_law', 'professional_medicine', 'professional_psychology', 'public_relations', 'security_study', 'sociology', 'sports_science', 'traditional_chinese_medicine', 'virology', 'world_history', 'world_religions']\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    count = 0\n",
    "    choices_head = ['A', 'B', 'C', 'D']\n",
    "    for task in task_list:\n",
    "        model_pre = model_info[0]\n",
    "        model = model_info[1]\n",
    "        with open(f\"./output/cmmlu/{model}/pretrained____data__home__chensh__data__huggingface_model__{model_pre}__{model}_cmmlu_{task}.jsonl\", 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        for i in range(len(json_file)):\n",
    "            resps = np.array(json_file[i]['resps'])\n",
    "            log_probability = resps[:,0,0]\n",
    "            probability = np.exp(log_probability)\n",
    "            sum_probability = np.sum(probability)\n",
    "            probability = probability / sum_probability\n",
    "            score = probability[json_file[i]['target']] if json_file[i]['acc'] == 1 else 0\n",
    "\n",
    "            if model_index == 0:\n",
    "                output_data.append(\n",
    "                    {\"question\":json_file[i]['doc']['Question'].strip() + \"\".join([\"\\n\" + choices_head[j] + \". \" + json_file[i]['doc'][choices_head[j]] for j in range(4)]) + \"\\n答案：\", \"scores\": {model_pre+\"/\" + model: score}}\n",
    "                )\n",
    "            else:\n",
    "                output_data[count][\"scores\"][model_pre+\"/\" + model] = score\n",
    "                count += 1\n",
    "                \n",
    "train_split_index = random.sample(range(len(output_data)), len(output_data))\n",
    "output_data = [output_data[index] for index in train_split_index]\n",
    "train_split = output_data[:int(0.7* len(output_data))]\n",
    "test_split = output_data[int(0.7* len(output_data)):]\n",
    "\n",
    "\n",
    "# output training data as a json file\n",
    "with open(os.path.join(output_file_path, 'cmmlu_train.json'), 'w') as f:\n",
    "    json.dump(train_split, f)\n",
    "\n",
    "with open(os.path.join(output_file_path, 'cmmlu_test.json'), 'w') as f:\n",
    "    json.dump(test_split, f) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cce83a",
   "metadata": {},
   "source": [
    "# MATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e57ac78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(52)\n",
    "\n",
    "\n",
    "# for cmmlu (multiple choice problem)\n",
    "\n",
    "task_list = [\"minerva_math_prealgebra\"]\n",
    "\n",
    "output_data = []\n",
    "for model_index, model_info in enumerate(model_list):\n",
    "    count = 0\n",
    "    for task in task_list:\n",
    "        model_pre = model_info[0]\n",
    "        model = model_info[1]\n",
    "        with open(f\"./output/MATH/{model}/pretrained____data__home__chensh__data__huggingface_model__{model_pre}__{model}_{task}.jsonl\", 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "        for i in range(len(json_file)):\n",
    "            if model_index == 0:\n",
    "                score = json_file[i]['exact_match']\n",
    "                output_data.append(\n",
    "                    {\"question\": json_file[i]['doc']['problem'], \"scores\": {model_pre+\"/\" + model: score}}\n",
    "                )\n",
    "            else:\n",
    "                score = json_file[i]['exact_match']\n",
    "                output_data[count][\"scores\"][model_pre+\"/\" + model] = score\n",
    "                count += 1\n",
    "\n",
    "\n",
    "# output training data as a json file\n",
    "with open(os.path.join(output_file_path, 'MATH_prealgebra.json'), 'w') as f:\n",
    "    json.dump(output_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9dd759a8e3a4a1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "# Get ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54200d44b45a60f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T02:04:47.184347Z",
     "start_time": "2024-04-18T02:04:47.112839Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mistralai/Mistral-7B-v0.1</th>\n",
       "      <td>29.878049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta-math/MetaMath-Mistral-7B</th>\n",
       "      <td>31.829268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teknium/OpenHermes-2.5-Mistral-7B</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WizardLM/WizardMath-7B-V1.1</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>itpossible/Chinese-Mistral-7B-v0.1</th>\n",
       "      <td>17.682927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HuggingFaceH4/zephyr-7b-beta</th>\n",
       "      <td>11.707317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cognitivecomputations/dolphin-2.6-mistral-7b</th>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NousResearch/Hermes-2-Pro-Mistral-7B</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cognitivecomputations/dolphin-2.2.1-mistral-7b</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abhishekchohan/mistral-7B-forest-dpo</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta-llama/Meta-Llama-3-8B</th>\n",
       "      <td>37.073171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shenzhi-wang/Llama3-8B-Chinese-Chat</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cognitivecomputations/dolphin-2.9-llama3-8b</th>\n",
       "      <td>53.841463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta-llama/Llama-2-13b-hf</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        0\n",
       "mistralai/Mistral-7B-v0.1                       29.878049\n",
       "meta-math/MetaMath-Mistral-7B                   31.829268\n",
       "teknium/OpenHermes-2.5-Mistral-7B                0.000000\n",
       "WizardLM/WizardMath-7B-V1.1                      0.000000\n",
       "itpossible/Chinese-Mistral-7B-v0.1              17.682927\n",
       "HuggingFaceH4/zephyr-7b-beta                    11.707317\n",
       "cognitivecomputations/dolphin-2.6-mistral-7b    45.000000\n",
       "NousResearch/Hermes-2-Pro-Mistral-7B             0.000000\n",
       "cognitivecomputations/dolphin-2.2.1-mistral-7b   0.000000\n",
       "abhishekchohan/mistral-7B-forest-dpo             0.000000\n",
       "meta-llama/Meta-Llama-3-8B                      37.073171\n",
       "shenzhi-wang/Llama3-8B-Chinese-Chat              0.000000\n",
       "cognitivecomputations/dolphin-2.9-llama3-8b     53.841463\n",
       "meta-llama/Llama-2-13b-hf                        0.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "output_file_path = \"./datasets/split2_model7\"\n",
    "with open(os.path.join(output_file_path, 'javascript.json'),'r') as f: \n",
    "    output_data = json.load(f)\n",
    "\n",
    "correct_dict = {\n",
    "'mistralai/Mistral-7B-v0.1': 0,\n",
    "'meta-math/MetaMath-Mistral-7B' :0, \n",
    "'teknium/OpenHermes-2.5-Mistral-7B':0,\n",
    "'WizardLM/WizardMath-7B-V1.1':0,\n",
    "'itpossible/Chinese-Mistral-7B-v0.1':0,\n",
    "'HuggingFaceH4/zephyr-7b-beta':0,\n",
    "'cognitivecomputations/dolphin-2.6-mistral-7b':0,\n",
    "'NousResearch/Hermes-2-Pro-Mistral-7B':0,\n",
    "'cognitivecomputations/dolphin-2.2.1-mistral-7b':0,\n",
    "'abhishekchohan/mistral-7B-forest-dpo':0,\n",
    "\"meta-llama/Meta-Llama-3-8B\":0,\n",
    "\"shenzhi-wang/Llama3-8B-Chinese-Chat\":0,\n",
    "\"cognitivecomputations/dolphin-2.9-llama3-8b\":0,\n",
    "\"meta-llama/Llama-2-13b-hf\": 0,\n",
    "}\n",
    "\n",
    "# print(output_data[0])\n",
    "\n",
    "data_size = len(output_data)\n",
    "print(data_size)\n",
    "\n",
    "for item in output_data:\n",
    "    scores = item['scores']\n",
    "    for key, score in scores.items():\n",
    "        # if score > 0: \n",
    "        correct_dict[key] += score / data_size * 100\n",
    "\n",
    "dataframe = pd.DataFrame.from_dict(correct_dict, orient='index')\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b85bfc1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
