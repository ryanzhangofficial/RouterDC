run_name: "baseline"
seed: 43

# You need to provide models in ascending order by parameter count for MESS+ to work correctly.
model_zoo:
  meta-llama/Llama-3.2-1B-Instruct:
    category: "small"
    gpu_indices: [0]
    max_seq_len: 2048
    gpu_memory_utilization: 0.12
    quantization: null
  unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit:
    category: "medium"
    gpu_indices: [0]
    max_seq_len: 2048
    gpu_memory_utilization: 0.15
    quantization: "bitsandbytes"
  unsloth/Llama-3.3-70B-Instruct-bnb-4bit:
    category: "large"
    gpu_indices: [0]
    max_seq_len: 2048
    gpu_memory_utilization: 0.68
    quantization: "bitsandbytes"

classifier_model:
  model_id: "answerdotai/ModernBERT-base"
  epochs: 3
  learning_rate: 0.0767
  weight_decay: 0.01
  momentum: 0.95
  batch_size: 128
  max_length: 128
  warmup_ratio: 0.1
  threshold: 0.5
  dropout_rate: 0.1
  freeze_bert_layers: true
  memory_size: 0
  memory_strategy: random # This only works when memory_size > 0
  reset_optimizer: false
  regularization_lambda: 0.0
  gpu_index: 0
  disable_tqdm: false
  # Path uses the "classifier" folder as root. Alternatively, you can specify an absolute path.
  checkpoint_path: "checkpoints/boolq"
  use_pretrained_classifier: true
  scoring_method: raw
  validation_dataset_size: 0.1
  generate_training_dataset: false
  write_training_dataset_to_disk: false

algorithm:
  router_encoder: "microsoft/mdeberta-v3-base"
  hidden_state_dim: 768
  node_size: 3
  similarity_function: "cos"
  router_checkpoint_path: "/dss/dssfs04/lwp-dss-0002/pn72yi/pn72yi-dss-0000/go76xom2/RouterDC/results/clw_1_slw_0_tk_3_lk_3_lr_5e-5_steps_1000_seed_1/final_model.pth"

lm_eval:
  # The run name will be the first benchmark in the list below.
  benchmarks: ["boolq"]
  num_repeats: 1
  limit_num_samples: null
  # TODO: Make sure we disable eager mode for the actual experiments to be as energy efficient as possible.
  enforce_eager: false
  write_to_disk: false
